# Copyright (c) 2025 SandAI. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from bisect import bisect_left
from collections import defaultdict
from dataclasses import dataclass

import torch

import magi_attention
from magi_attention.common import AttnRange, AttnRanges
from magi_attention.utils import is_list_value_all


@dataclass(repr=False)
class AttnArg:
    q_ranges: AttnRanges
    k_ranges: AttnRanges
    is_causal_mapping: list[bool]

    # REVIEW(xiaowu): Is shard_seqlen_q an appropriate name?
    shard_seqlen_q: int

    # total area of the attn arg, -1 means unknown
    total_area: int = -1

    # NOTE: The following variables are automatically generated by __post_init__ and are used as the arguments of ffa.
    # max_seqlen_q: int
    # max_seqlen_k: int
    # skip_attn: bool
    # q_ranges_tensor: torch.Tensor
    # k_ranges_tensor: torch.Tensor
    # is_causal_mapping_tensor: torch.Tensor
    # out_zero_fill_ranges: list[tuple[int, int]]

    def __post_init__(self):
        # shape check
        assert len(self.q_ranges) == len(self.k_ranges) == len(self.is_causal_mapping)
        # assert len(self.q_ranges) == len(self.k_ranges) == len(self.attn_type_map)

        # init out zero-fill ranges for fwd out correction
        # TODO: put this logic into kernel
        self._init_out_zero_fill_ranges()

        # filter empty, and overwrite the original inputs
        self._filter_out_empty_slice()

        # init fwd ffa args dict
        # NOTE: we need to init fwd args first before bwd args
        self._init_ffa_fwd_args_dict()

        # init ffa args for bwd
        self._init_ffa_bwd_args_dict()

    def _filter_out_empty_slice(self) -> None:
        filtered_q_ranges = AttnRanges()
        filtered_k_ranges = AttnRanges()
        filtered_is_causal_mapping: list[bool] = []

        # filter out k_ranges with seqlen == 0
        for q_range, k_range, is_causal_mapping in zip(
            self.q_ranges, self.k_ranges, self.is_causal_mapping
        ):
            if not k_range.is_empty():
                filtered_q_ranges.append(q_range)
                filtered_k_ranges.append(k_range)
                filtered_is_causal_mapping.append(is_causal_mapping)

        # overwrite the original inputs
        (
            self.q_ranges,
            self.k_ranges,
            self.is_causal_mapping,
        ) = (
            filtered_q_ranges,
            filtered_k_ranges,
            filtered_is_causal_mapping,
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # shape check
            assert (
                len(self.q_ranges) == len(self.k_ranges) == len(self.is_causal_mapping)
            )
            # check non-empty k ranges
            for k_range in self.k_ranges:
                assert not k_range.is_empty()

            # check non-overlapped q ranges
            assert self.q_ranges.is_non_overlap() or is_list_value_all(
                self.is_causal_mapping, False
            )

    def _init_out_zero_fill_ranges(self) -> None:
        shard_q_ranges = AttnRanges.from_ranges([[0, self.shard_seqlen_q]])
        self.out_zero_fill_ranges = AttnRanges.find_hole_ranges(
            shard_q_ranges,
            self.q_ranges,
            is_self_merged=True,
        ).to_naive_ranges()

    def _init_ffa_fwd_args_dict(self) -> None:
        # init `skip_attn_fwd` flag
        batch_size_fwd = len(self.q_ranges)
        self.skip_attn_fwd = batch_size_fwd == 0

        # init `disable_fwd_atomic_reduction` flag
        self.disable_fwd_atomic_reduction = self.q_ranges.is_non_overlap()

        # init tensors
        q_ranges_tensor_fwd = self.q_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        k_ranges_tensor_fwd = self.k_ranges.to_tensor(
            device=torch.cuda.current_device()
        )
        mask_type_tensor_fwd = torch.tensor(
            self.is_causal_mapping,
            dtype=torch.int32,
            device=torch.cuda.current_device(),
        )

        # sanity check
        if magi_attention.is_sanity_check_enable():
            # check tensor shape
            if not self.skip_attn_fwd:
                assert q_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert k_ranges_tensor_fwd.shape == torch.Size([batch_size_fwd, 2])
                assert mask_type_tensor_fwd.shape == torch.Size([batch_size_fwd])

        # init max seqlen
        if self.skip_attn_fwd:  # no calc needed
            max_seqlen_q_fwd = 0
            max_seqlen_k_fwd = 0
        else:
            max_seqlen_q_fwd = self.q_ranges.max_seqlen
            max_seqlen_k_fwd = self.k_ranges.max_seqlen

        self.ffa_fwd_args_dict = dict(
            q_ranges=q_ranges_tensor_fwd,
            k_ranges=k_ranges_tensor_fwd,
            attn_type_map=mask_type_tensor_fwd,
            max_seqlen_q=max_seqlen_q_fwd,
            max_seqlen_k=max_seqlen_k_fwd,
        )

    def _init_ffa_bwd_args_dict(self) -> None:
        # NOTE: the feature `refactor_bwd_args` now is only experimental
        # so here's an env variable switch to turn it on/off
        if magi_attention.is_refactor_bwd_args_enable():
            # refactor ranges and types for bwd dkv load-store efficiency
            (
                self.q_ranges_bwd,
                self.k_ranges_bwd,
                self.is_causal_mapping_bwd,
            ) = self._refactor_bwd_ranges_and_types()

            # init skip attn flag
            batch_size_bwd = len(self.q_ranges_bwd)
            self.skip_attn_bwd = batch_size_bwd == 0

            # init tensors
            q_ranges_tensor_bwd = self.q_ranges_bwd.to_tensor(
                device=torch.cuda.current_device()
            )
            k_ranges_tensor_bwd = self.k_ranges_bwd.to_tensor(
                device=torch.cuda.current_device()
            )
            mask_type_tensor_bwd = torch.tensor(
                self.is_causal_mapping_bwd,
                dtype=torch.int32,
                device=torch.cuda.current_device(),
            )

            # sanity check
            if magi_attention.is_sanity_check_enable():
                # check tensor shape
                if batch_size_bwd > 0:
                    assert q_ranges_tensor_bwd.shape == torch.Size([batch_size_bwd, 2])
                    assert k_ranges_tensor_bwd.shape == torch.Size([batch_size_bwd, 2])
                    assert mask_type_tensor_bwd.shape == torch.Size([batch_size_bwd])

            # init max seqlen of fwd and bwd
            if self.skip_attn_bwd:  # no calc needed
                max_seqlen_q_bwd = 0
                max_seqlen_k_bwd = 0
            else:
                max_seqlen_q_bwd = self.q_ranges_bwd.max_seqlen
                max_seqlen_k_bwd = self.k_ranges_bwd.max_seqlen

            self.ffa_bwd_args_dict = dict(
                q_ranges=q_ranges_tensor_bwd,
                k_ranges=k_ranges_tensor_bwd,
                attn_type_map=mask_type_tensor_bwd,
                max_seqlen_q=max_seqlen_q_bwd,
                max_seqlen_k=max_seqlen_k_bwd,
            )
        else:
            # just copy args from fwd
            self.skip_attn_bwd = self.skip_attn_fwd
            self.q_ranges_bwd = self.q_ranges
            self.k_ranges_bwd = self.k_ranges
            self.is_causal_mapping_bwd = self.is_causal_mapping

            self.ffa_bwd_args_dict = self.ffa_fwd_args_dict

    def _refactor_bwd_ranges_and_types(
        self,
    ) -> tuple[AttnRanges, AttnRanges, list[bool]]:
        """Refactor bwd ffa args including q,k ranges and mask types
        from fwd ffa args (i.e. original ffa args) for bwd dkv load-store efficiency
        TODO:
            1. consider restricting the "refactor degree"
                to avoid insufficient number of blocks in grid to be parallelized
            2. support causal mask
            3. test the actual performance together with
                the top-p minhp dispatcher with IOU affinity considered
        """
        # get fwd ranges and mask type
        q_ranges_fwd, k_ranges_fwd, is_causal_mapping_fwd = (
            self.q_ranges,
            self.k_ranges,
            self.is_causal_mapping,
        )

        # TODO: support causal mask
        assert is_list_value_all(
            is_causal_mapping_fwd,
            False,
            allow_empty=True,
        ), f"Only support all full masks for now, but got {is_causal_mapping_fwd=}"

        # init two map q_range->k_ranges and k_range->q_ranges
        map_slice_q_range_to_k_ranges: defaultdict[AttnRange, AttnRanges] = defaultdict(
            AttnRanges
        )
        map_slice_k_range_to_q_ranges: defaultdict[AttnRange, AttnRanges] = defaultdict(
            AttnRanges
        )

        # get boundary list from fwd k_ranges
        k_ranges_boundary: list[int] = k_ranges_fwd.points

        for q_range_fwd, k_range_fwd in zip(q_ranges_fwd, k_ranges_fwd):
            # get start and end of k_range of current slice
            slice_k_range_start, slice_k_range_end = (
                k_range_fwd.start,
                k_range_fwd.end,
            )

            # find start and end index in the boundary list
            boundary_left_index = bisect_left(k_ranges_boundary, slice_k_range_start)
            boundary_right_index = bisect_left(k_ranges_boundary, slice_k_range_end)

            # split slice in k_range dimention with boundary list
            for boundary_index in range(boundary_left_index, boundary_right_index):
                boundary_start, boundary_end = (
                    k_ranges_boundary[boundary_index],
                    k_ranges_boundary[boundary_index + 1],
                )

                # add split_k_range->q_range to map
                k_range_this_slice = AttnRange(start=boundary_start, end=boundary_end)
                map_slice_k_range_to_q_ranges[k_range_this_slice].append(q_range_fwd)

        # sort k_range->q_ranges map with k_range.start
        k_range_q_ranges_tuples: list[tuple[AttnRange, AttnRanges]] = sorted(
            map_slice_k_range_to_q_ranges.items(), key=lambda t: t[0].start
        )

        # Convert the content in the k_range->q_ranges map to the q_range->k_ranges map.
        for k_range, q_ranges in k_range_q_ranges_tuples:
            q_ranges = q_ranges.merge()
            for q_range in q_ranges:
                map_slice_q_range_to_k_ranges[q_range].append(k_range)

        # sort q_range->k_ranges map with q_range.start
        q_range_k_ranges_tuples: list[tuple[AttnRange, AttnRanges]] = sorted(
            map_slice_q_range_to_k_ranges.items(), key=lambda t: t[0].start
        )

        # initial ranges of bwd
        q_ranges_bwd, k_ranges_bwd = AttnRanges(), AttnRanges()

        # merge k_ranges in the map and append to ranges of bwd
        for q_range, k_ranges in q_range_k_ranges_tuples:
            k_ranges = k_ranges.merge()
            for k_range in k_ranges:
                q_ranges_bwd.append(q_range)
                k_ranges_bwd.append(k_range)

        is_causal_mapping_bwd = [False] * len(q_ranges_bwd)

        return (
            q_ranges_bwd,
            k_ranges_bwd,
            is_causal_mapping_bwd,
        )

    def to_ffa_args(self, is_bwd: bool = False) -> dict:
        return self.ffa_bwd_args_dict if is_bwd else self.ffa_fwd_args_dict

    def can_skip(self, is_bwd: bool = False) -> bool:
        return self.skip_attn_bwd if is_bwd else self.skip_attn_fwd

    def __repr__(self) -> str:
        return (
            f"AttnArg(q_ranges={self.q_ranges}, k_ranges={self.k_ranges}, is_causal_mapping={self.is_causal_mapping}, "
            f"shard_seqlen_q={self.shard_seqlen_q}, total_area={self.total_area}"
        )


@dataclass
class AttnCalcMeta:
    local_attn_arg: AttnArg
    remote_attn_args_list: list[AttnArg]

    @property
    def overlap_degree(self) -> int:
        return len(self.remote_attn_args_list)

    def __post_init__(self):
        pass
